%!TEX root = ../report.tex

\chapter{Introduction}

Assessment of the knowledge acquired by the students is one of the most important aspects of the learning process. Different forms of assessments that exist today include multiple choice questions, fill-in-the-blanks, essay questions and short answer questions. Prior works have shown that multiple choice questions and fill-in-the-blanks fail to capture the vital aspects of the acquired knowledge such as reasoning and self-explanation \cite{Wang2008}. In contrast, questions which require the students to construct responses in natural language have been found to be more effective in assessing their grasp on the subject matter \cite{Roy2016a}. Essay questions and short answer questions belong to this category. Short answer questions are characterized by the following aspects\cite{Burrows2015}:

\begin{itemize}
	
	\item the question must require a response that recalls external knowledge instead of requiring the answer to be recognized from within the question
	
	\item the question must require a response given in natural language
	
	\item the answer length should be roughly between one phrase and one paragraph
	
	\item the assessment of the responses should focus on the content instead of writing style
	
	\item the level of openness in open-ended versus close-ended responses should be restricted with an objective question design 
	
\end{itemize}

Automatic short answer grading essentially deals with using computational methods to predict the grades for students' answers, thus cutting down the effort and time of teachers / professors. Many automated approaches have been proposed in the past for grading short answer questions. These methods compare how similar the students' answers are to the one provided by the teacher or professor and assign a grade proportional to the magnitude of similarity \cite{Mohler2011}. Different approaches of computing these similarity measures include handcrafted pattern matching, automatic pattern matching, lexical similarity (how particular words effect other words), semantic similarity (deals with the meaning of sentences), and entailments (whether one sentence leads to another without any contradiction). Fig \ref{auto_workflow} shows a general workflow of automatic short answer grading as a pipeline. After creating a dataset of all the students' answers and model answers written by the teachers, useful features are extracted from the answers using natural language processing techniques. A model is developed based on these features to predict the scores and deployed for use. Such a model learns only once from the available set of labeled data and there is no feedback mechanism to fine-tune the parameters based on the wrong predictions after the deployment.

\vspace{3mm}
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.6]{images/simpler_model_1}
	\caption{Workflow of automatic short answer grading \cite{Burrows2015}.}
	\label{auto_workflow}
\end{figure}

This work proposes an approach where a generic scoring model tries to learn this task of measuring the correctness of each answer continuously with a human in the loop. During the learning stage, the system selects the best samples for the human to grade which would eventually contribute to its knowledge base. Thus, it tries to reduce the human effort of going through all the answers while improving it's understanding of the problem on a cyclical and iterative basis. \\

\begin{figure}[h!]
	\includegraphics[width=\textwidth]{images/active_learning}
	\caption{Workflow of active learning \cite{Settles2010}.}
	\label{ac_workflow}
\end{figure}

Active learning seems to be the best choice for this task as it actively queries the human for grades of the samples it is most uncertain of. Fig \ref{ac_workflow} illustrates a typical workflow of active learning. It is a subfield of machine learning which works under the hypothesis that if the learning algorithm is allowed to choose the data from which it learns it will perform better with less labeled data and training \cite{Settles2010}. Such a model queries a user / human expert for the labels of certain data samples in such a way that it can learn to produce the desired outputs with higher accuracy. By actively selecting the data samples to label, it reduces a considerable amount of labeled training samples, thus, alleviating the problem of insufficient labeled data which is prevalent in supervised learning approaches. In addition, it would be a solution to deal with the diversity in answers as there is generally no single best response for an open-ended question.

%\lipsum[1-5]
\section{Motivation}

Limited availability of teachers, online learning platforms, and individual or group study sessions done outside classrooms necessitated quick and efficient assessment of free text responses. In addition, almost 30\% of the teachers' time is spent on grading the assessments \cite{mason2002}.Computer assisted assessment / automatic grading evolved as a solution to this problem and a lot of research has been done on automating the grading of essay\cite{Higgins2004} and short answer responses\cite{Leacock2003, Pulman2005, Mohler2009}. The focus of this work would be on improving the existing solutions to grade short answer questions.

%\subsection{...}

%\lipsum[6-10]

%\subsection{...}


\section{Challenges and Difficulties}

The methods discussed so far belong to a learning paradigm called supervised learning where the right answer for every question is available \cite{Horbach2016}. Though these approaches were able to produce decent results, they suffer from many shortcomings such as;

\begin{itemize}
	
	\item the failure to capture the different wordings/phrasing of the students while trying to answer the short answer questions. It is obvious that anticipating all different ways of answering his questions is practically impossible for the professor. 
	
	\item lack of sufficient amount of labeled training data in the domain to learn the models. Reasons include annotation cost, privacy, availability, and the quality of the correct answers. 
	
	\item inability to capture consistent patterns of misunderstandings among students. Ability to recognize such patterns would enable the automated systems to provide useful feedback to students as to why there was a reduction in marks awarded.
	
	\item accounting for small deviations in the answers which might affect the whole meaning of the sentence (for ex. in mathematical terms, though each and every word of the student's answer align with that of the professor, a small negation or inverse operation would change the whole meaning).
	
	\item finding a way to understand the underlying concept of various students' answers and bagging the similar ones (or the right and wrong ones separately) is also a very tedious task. 
	
	\item being a passive learner, these models learn the rules once and apply them on new input answers. Thus, it would be very difficult to achieve robustness when applied to new data over time. \\
	
\end{itemize}


%\subsection{...}

%\lipsum[11-15]

%\subsection{...}

%\subsection{...}



\section{Problem Statement}

This work differs from the approach illustrated in Fig \ref{auto_workflow} by incorporating a feedback mechanism which allows the model to learn continuously based on new labeled input samples. As illustrated in Fig \ref{ac_simple_workflow}, we propose a model which implements active learning for the task of short answer grading.

\vspace{5mm}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{images/simpler_model_ac}
	\caption{Workflow of active learning in automatic short answer grading} %\cite{Burrows2015}.}
	\label{ac_simple_workflow}
\end{figure}

\begin{itemize}
\vspace{3mm}
	\item Active learning will be evaluated as a potential solution to overcome the deficits of automated short answer grading in this work. Thus, issues such as high cost of labeled data, understanding common misconceptions among the students, lexical diversity in answers, and inferior performance will be addressed in this work.   
	\item Various natural language processing techniques such as semantic similarity, entailment and the best features to be extracted from the answers will be experimented for the task of short answer grading. 
	\item Active learning strategies will be evaluated based on aspects such as seed selection, instance sampling, and batch size. The best strategies will be selected for the implementation of the final model.
	\item A generic interactive model for assessments in different domains will be implemented based on the results of aforementioned experiments.
	\item All techniques will be analyzed on performance, reliability, amount of human action(clicks) required, flexibility for different domains, and user satisfaction.  
\end{itemize} 